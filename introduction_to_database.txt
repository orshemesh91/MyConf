Introduction
------------

Storage access times:
    L1 cache - 0.5 ns
	L2 cache - 7 ns
	DRAM -  100 ns
	SSD - 150,000 ns = 150 micro sec
	HDD - 10,000,000 ns = 10 milli sec
	Network storage - 30,000,000 ns = 30 milli sec
	tape archives - 1,000,000,000 ns = 1 sec

DBMS - database management system - allows definition, creation, querying, update and administration of databases
DML - data manipulation languages:
    Procedural (Relational Algebra) - the query specifies the high level strategy the DBMS should use to find the desired result.
        Relational algebra operators:
            Select
            Projection
            Union
            Intersection
            Difference
            Product
            Join
        Extra operators:
            Rename
            Assign
            Aggregate
            Sort
            Division
    Non-Procedural (Declarative) (Relational Calculus) - the query specifies only what data is wanted and not how to find it

SQL - Structured Query Language
-------------------------------
DML - Data Manipulation Language - actions on data
DDL - Data Definition Language - creating tables
DCL - Data Control Language - security and permissions
Aggregates - can only be used in the SELECT output list
    AVG(col)
    MIN(col)
    MAX(col)
    SUM(col)
    COUNT(col)
    example:
        SELECT COUNT(login) AS cnt FROM student WHERE login LIKE '@sc';
        SELECT COUNT(*) AS cnt FROM student WHERE login LIKE '@sc';
        SELECT COUNT(1) AS cnt FROM student WHERE login LIKE '@sc'; TODO Kopzon - why all 3 are the same?
    GROUP BY
        Non aggregated values in SELECT output, must appear in GROUP BY clause
        example:
            SELECT AVG(s.gpa), e.cid, s.name FROM enrolled AS e, student AS s WHERE e.sid = s.sid GROUP BY e.cid, s.name;
    HAVING
        Filters results based on aggregation computation (like WHERE clause for a GROUP BY)
        example:
            SELECT AVG(s.gpa) AS avg_gpa. e.cid FROM enrolled AS e, student AS s WHERE e.sid = s.sid GROUP BY e.cid HAVING avg_gpa > 3.9;
String operations:
    LIKE
        % - matches any substring (including empty ones)
        _ - matches aby one char
        example SELECT * FROM student AS s WHERE s.login LIKE 'kookoo%bomba_yal'
    SUBSTRING
        SELECT SUBSTRING(name,0,5) AS abbrv_name FROM student WHERE sid = 55843
    UPPER/LOWER
        SELECT * FROM student as s WHERE UPPER(e.name) LIKE 'KAN%'
    Concatinate
        ||
        +
        CONCAT(,)
Date and Time operatiopns:
    veries in diferent flavores of sequl
Output redirection:
    create new table
        SELECT DISTINCT cid INTO CourseIds FROM ENROLLED;
    add to existing table
        INSERT INTO CourseIds (SELECT DISTINCT cid FROM enrolled)
    ORDER BY <column> [ASC|DESC]
        SELECT sid FROM enrolled WHERE cid = '15-721' ORDER BY grade DESC, sid ASC (note that grade is not even part of the output)
    LIMIT <count> OFFSET [offset]
Window Functions:
    Like an aggregation but tuples are not grouped into a single output (you get the whole tuple in the output).
        SELECT ... FUNC-NAME(...) OVER(...) FROM tableName
        where FUNC-NAME can be:
            Aggregation functions...
            ROW_NUMBER() - # of the current row
            RANK() - order of the current row
Common table expressions
    useful to avoid query nasting.
        WITH cteName (col1, col2) AS (SELECT ...) SELECT * FROM cteName - col1 and col2 and bind to the temp table columns names
    can be used for recusrions:
        WITH RECURSIVE cteSource (counter) AS (
            (SELECT 1)
            UNION ALL
            (SELECT counter + 1 FROM cteSource WHERE counter < 10)
        )
        SELECT * FROM cteSource;

Memory Management
-----------------
Why not using mmap()
    The OS doesn't know what really does the application do, so swap stalls are very heavy. A good workaround for this issue is:
        madvice() - tell the OS how you expect to read the pages
        mlock() - mark memory ranges that cannot be paged out
        msync() - flush memory
    still this isn't a good solution for multiple writers. (Full usage - monetDC, LMDB, ravenDB, levelDB elasticsearch)
    OS is general purpose track, while our system is a Porche or Ferrari
    TODO Kopzon - managing our own file system in a big pain in the ass and can give not more than 10% performance enhancement. (is it true?)

How the DBMS represents the DB in files on disk?
    Page contains unique sort of data (tuples, meta-data, indexes, log records...)    
    indirection layer to map page id to file and offset.
    DB page size is 512B-16KB (ORACLE, IBM DB2 and SQLite - 4KB) (MySQL - 16KB high end) high end systems usually give choice with DB page size per type
    Hardware pages are 4KB atomic
    page directory - lookup table for pages on disk
    page contents:
        Header:
            page size
            checksum
            BDMS version
            Transaction visibility TODO Kopzon - what is it?
            Compression info
        1st option:
            Slot array - mapping layer from a slot to a tuple (slot array growing from beginning to the end and tuples grow from end to beginning)
            Tuples
        2nd option:
            Log structured - Append records to a log file when DB is modified. (good for writes, bad for reads, but you can periodically build indexes and compact logs)
                Insert - store the entire tuple
                Delete - mark the tupl as deleted
                Update contain the delta of the modified attribute
    GC compacts the tuples in a page
    tuple contents:
        header
        attributes
        BLOB - Binary Large OBject - is a tuple which's content is too big. Can be handles by:
            few pages with indirection (like a list of pages)
            within an external file (DBMS can't manipulate these files)
    normalization - the way you split up the DB across different tables
    denormalization - pre joining - create a join table instead of two tables (cloud spanner from Google - protobuf nested tables use denormalization)
    Table schema - meta data of a table in the DB (can sit inside the DB itself or rely on external FS)
    Transaction types:
        OLTP - Online transaction processing - simple mostly write queries, small amount of data involved
        OLAP - Online analytical processing - complex mostly read queries, large amount of data
        HTAP - Hybrid transactional analytical processing
    mongoDB, bigTable and kasandra are OLTP optimized and are noSQL systems and this is why the are able to scale TODO Kopzon - why and what does it mean?
    storage models:
        n-ary (row store) - all the attributes of a single tuple are stored continuesly in a single page - best for OLTP workloads
        DSM (column store) - Decomposition Storage Model - best for OLAP workloads (analytics). How do we match different attributes of same tuple?
            fixed length offsets - issue with variable length attributes
            embedded tuple ids - waste of space
        get 4KB page from SSD - 150micro, scan 4KB in INT32 jumps is 33micro (3Ghz processor, 10clock cycles per loop)? TODO Kopzon - check
        It is a common solution to store the DB with a row transactional store (OLTP) for real time transaction based applications and when the data starts to be less relevant (in time for example) it is copied to a backend column analytical store (OLAP).

How the DBMS manages it's memory and moves data back and forth from disk?
    allocate a memory region called buffer cache (frame pool) and bring needed pages from dist to it.
    Use page table to map page to frame in the pool.
    save dirty and reference bits on frames to control correct swapping.
    lock - high level transaction syncronization mechanism (should be able to roll back transactions etc...)
    latch - low level lock for protecting data resources from multi-threaded corruption.
    page directory - where is the page in the disk (must be durable)
    page table - where is the page in the buffer pool (don't have to be durable)
    pre-fetching - OS prefetchs pages using simple locality heuristic. The DBMS can pre-fetch non-sequential pages based on table indexes and query requirements.
    scan sharing - allow multiple queries attach to a single table cursor that scans the table. (queries don'thave to be exactly the same)
    result caching - save query result in cache to output it on demand for the same query in the future
    buffer cache bypass - for sort/ large sequential queries - allocate temporary memory for pages for the specific query to reduce buffer cache cooling (with regard to other queries that use the buffer cache and made it "hot" for them)
    most DBMS use O_DIRECT (direct IO) to avoid using the OS page cache.
    page replacement policy
        LRU
            time stamp per page in page table + scan all pages in page table vor every page removal operation (wase of time to iterrate)
            queue - add to the end, remove from the beginning + reinsert all pages that are reused (the queue lock is contended)
            clock - approximation of LRU - hold all pages in a cyrcular buffer. if page is referenced, update bRef to 1. when page should be swapped out the if the clock arrow points to a page with bRef = 1, assign bRef = 0 and go to the next one, otherwise swap this page. (no need for taking lock on cache hit, just update bRef)
            note:
                sequential flooding - cache trashing on one sequential query
        LRU-K
            track the lask K references to each page and remove from cache the one with the biggest AVG interval between timestamps
        Localization per query - hold page cache per query and not a global one
        Priority Hints - knowing the query structure we can save in cache the most frequently needed pages for the query

Hash Tables:
    Linear Search - on collision put the new key at the first available slot after it's optimal slot.
        Robin Hood hashing - each inserted key stores it's distance from it's optimal slot, and if new key's distance is greater then the old in the slot, they are swapped.
        CuKoo Hashing - multiple hash functions and hash tables. insert to the hash that doesn't have a collision. If there is a collision, replace the old key and reinsert it to another hash. Do it in a loop till you reinsert all key's or recognize you need to extent the hash table size.
        In case of resize we have to rebuild the entire table
    Chain hashing - each entry maps to a bucket of keys.
    Extendible hashing - Like chain hashing, but the pointers table  has a global counter that represents the amount of  bits to we have to look at when searcing for bucket (page) with data. When a bucket is getting full, we split only this bucket, double the pointer's table size, increasing the global bit counter and update the pointers (updating only 3 pages for resize)
    Linear haashing - splitting buckets based on the split pointer, double the slot array, add new hash function with modulo X2 from the previous hash function and remove the old hash function when the split pointer gets to the end of the slot array. To find a key use the first hash function and if the result is above the split pointer we may need to use the second hash function. split pointer can move back and slot arary can shrink by two when a splitted bucket gets empty

B-Trees:
    The main difference between B-Tree and a B+Tree is in B-Tree there are no duplicates of keys (data is all over the tree and not only in leaves) so it is more compact, but it is not used since it is much more efficient to multi-thread over B+Tree (data in only in leaves so taking latch will not block other leaves from being updated)
    why B+trees are better than hash tables for indexing?
        tuples are sorted in the key order on disk - range queries are faster and require less page accesses.
        can do a partial key search (wild card searches e.g <A,*>)
    variable key sizes - use in-page indirection table that grows from the beginning of the node (page) while the key values + data themselves grow from the end (like  a slot array) nice trick - save the first character of the real key in the slot array entries to avoid ram cache garbeging and redundent lookups.
    node optimizations:
        prefix compression - most of the keys in a leaf are very similar (at least same prefix), so there is no point in storing the common prefix multiple times
        suffix truncation - in the inner nodes of the B+Tree we can store only some prefix which is just enough to navigate threw the tree
        bulk insert - if you have all the keys ahead of time you can sort them and build a B+Tree from buttom to up(top) it is much faster (O(n) and not O(n*log(n))
        pointer swizzling - if a page is pinned, store the direct pointer to a frame in the buffer cache instead of the page id to reduce page table access
    non-unique keys:
        can be handled "as usual" - reorder the relevant page and split if needed
        use an "overflow page". This overflow page can be sorted (slower to insert and maintain) or unordered.
    Partial indexes - can save partial key set - optimizes specific queries
    Covering index - can save partial tuple data inside the index itself - to avoid indirection to the tuple itself
    Index include columns - add some attributes to a Covering index
    Functional/Expression index - like a partial index build on some function or expression over the attributes (not the attribute itself)

Trie:
    used to determine whether a key exists or not to avoid searching by the index and potentially get multiple page misses
    Trie is faster for point queries, B+ faster for scans
    horizontal compression - remove the values from the inner nodes, store only the pointers (the values order in each node's array is a predefined known convention you define)
    vertical compression - if there is only one key for some prefix, don't save the whole path till the end of the key string, point to the corresponding tuple immediately.
    Radix tree - Trie with vertical and horizontal compression.

Inverted index (full-text search index):
    stores a mapping of words to records that contain those words in the target attribute.

Multi-threading:
    OS mutex - is ~30ns for each acquire and release. It tries to test and set at the user space (futex) and if fails goes to the kernel mutex which reschedules the whole thread.
    std::atomic_flag latch - can use "while (latch.test_and_set()) { /*retry? yield? abort?*/ }" to have more control over the multi threading in the system
    Hash table - lock the entry you need, in linear search collision case lock the next node and release the previous. may first lock for read only the check whether the node contains the key we need and only if yes to take a write lock.
    B+Tree - latch crabbing/coupling - get parent latch, get child latch, release parent latch if safe (node not full on "add" node not half full at "delete") - use a stack of latches (queue is better from performance stand point). Same as in Hash tables, first use a read latch and only if we got to the leaf node (which takes a write latch) and it has to do a rebalance operation, only then re-take the needed write latches on the relevant parents.
    Lazy parent updates in B+Trees - have a global table with inner nodes that have to be updated due to previous splits. the split command only creates the new node and links it to the sibling leaf. only the next command that will get the parent's lock will update it. TODO Kopzon - so now we have to create another if statement on every inner node?!?
	Sibling pointer latches should kill the operation if stuck due to colission with vertical latches to avoid deadlocks.
	
JOIN Algorithms:
    Operator output:
	    data - save a brand new tuple with the needed attributes from the joined tables (can be optimized to filter out the redundant attributes if they are known from the query itself)
        record id (late materialization) - save a tuple with page id and record id (pointers) (not good for high scale systems)
    Nested Loop algorithm - if memory has B free blocks then bring B-2 blocks from the outer relation to memory and scan threw the inner relation blocks one by one (ont block for input and one for output). This one is mostly used when one of the relations fits the RAM
    Sort-Merge Algorithm - first sort the relations and then merge them (may need backtrack on the inner relation (save pointer of the first time you saw a value when incrementing the inner relation pointer))
        If the relation has an index that already sorted we don't have to do the sort phase.
        If we have an ORDER BY clause on the same attribute, then the JOIN's sorting operation output can be used for the ORDER BY calculation
	Hash Algorithm - put both relations to a hash table by same hash function (first the outer relation then the inner). then go over the buckets and output the join.
        optimization - build a bloom filter when creating the outer hash table.
            bloom filter - "one way" filter - lookup bit map which supports insert() and lookup() API.
        recursive partitioning - if a partition (buket of outer and inner relations) doesn't fit memory, the do another loop of hash with another hash function

Query Plan:
	Processing model:
        Iterator model (pipeline) - for each query operator we implement a next() function. Each tuple is propagated threw all query functions in a pipeline. Good for OLAP data model - complex read queries can be processed in pipeline giving a high threughput with relatively low memory consumption.
        Materialization model - each operator processes all tuples at once and outputs all at once. Can use hints from the DBMS to optimize per query. Good for OLTP data model - small amount of tuples involved so using smaller amount of functions invocations and branches. Good for in-memory systems.
        Vectorization model - like Iterator model but instead of each tuple propagate a block of tuples at once (batch processing). Best for OLAP
    Sequential scan optimizations:
        Prefetching
        Buffer pool bypass
        Paralleization
        Zone maps - for each page save a meta data (in the page itself or in another dedicated page) that gives hints which help queries not access pages they don't need. Good for OLAP since they mostly don't update the tuples and so don't have to update the zone maps syncroniousely all the time
        Late materialization - delay stiching togather tuples untill the upper part of the query plan - passing only offsets (pointers) if the upper parts don't really need the aggregation attribute from the lower parts.
        Heap clustering - use sorted indexes where able to
	Index scan:
        scan threw an index (can by multiple indexes and then an intersention or union between them). The DBMS Optimizer decides whether to use index or sequential scan.
        if the index is unclustered, first figure out all the tuples needed for the query, sort them by their page id and then bring them to memory by their page id order to avoid buffer cache garbeging
    Expression trees are the high level query plan. They are flexible but slow. High end systems know to take a query template, parameters for it and compile down at run time the query plan code to a dedicated function instead of having a bunch of generic functions with multiple switch case statements that match all tupes of queries.

TCO - Total Cost of Ownership - hardwre + sofrware + run time + maintenence + .....

Parallel vs distributed:
    Parallel is multithreading/multiresources on a single machine
    Distributed is across multiple servers
Processing parallelizm:
    inter query
    intra query
    inter operator
    intra operator
    exchange operator - inner DBMS operation which is responsible of gathering data together after multi-threaded execution.
storage parallelizm:
    basically the DBMS doesn't care about it and sits on anpther layer that controls the storage (e.g raid controller). The DBMS just controls the logical partitioning
        horizontal partitioning - sharding - different tupples of same table on different storage devices
	vertical partitioning - different attributes on different storage devices

Query optimization:
    rewrite the query to avoid redundant parts, estimate a cost of different possible query plans and choose the "cheepest" one.
    Predicate pushdown - move filters closer to the buttom of the evaluation tree (only f the filtering operation is cheeper than other operations in the query - which is mostly like this).
    Reorder filters from the most filtering to the least.
	Projection pushdown - don't propagate redundant data.
    Remove imposible/redundant predicates
	Join elimination - if both tables in the JOIN operation are evaluating to the same table
	Ignoring projections - TODO Kopzon leacture 14 39:45
    Merging predicates of the same attribute

Execution plan choose:
	MongoDB runs a lot of query plans in parallel, returns the output of the first that returns and saves this plan for this query in a catalog, so next time this query is invoked, the DBMS will know which plan to execute. When the table changes more than by 10%, the cached plan is removed and next time the query will be invoked it will run again multiple plans to check if there is a better one (simple but working heuristic).
    System catalog - saves metadata about the tables, indexes and statistics for different optomizations. ANALYZE - system statistics builder - can be run at night to update the histograms and catalog but not colide with user queries
        Attribute correlated data - maintain attribute correlation to estimate query plan cost more efficiently. high end systems have it
        Histogram unique value occurence - per table, per attribute save an histogram that shows how much ocurences each value has. high end system have it
        Sampling - have a smaller selective table (e.g only 1 from each 10 tuples) and assume the selectivities are similar to the original table

Concurency control:
    MySQL doesn't do transactions to be faster.
	ACID:
        Atomicity:
            trx finishes with commit or abort
            Logging - 
            Shadow paging - make a copy of every page the trx changes and arite them only at the trx commit. TODO Kopzon - how multiple page writes can be atomic? (old IBM approach)
        Consistency:
            trx consistency 
                later trxs see results of older trxs (app responsibility).
            data base consistency
                database follows integrity constraints (models the real world). If DB is consistent before a trx then it is consistent after it commits or aborts.
                    If transaction preserves consistency, every serializable schedule preserves consistency. DB ensures that resulting execution of trxs is equivalent to some serial execution of the trxs 
                    View serializability - 
                        weaker demand then conflict serializability but much harder to verify and DBs don't really implement it.
                        for each conflicted resource A of trxs T1 and T2, schedules S1 and S2 are view equivalent if in both schedules:
                            the two trxs read the same initial value
                            T1 sees T2.write(A) and vice versa
                            final value  of A is written by the same trx
                    Pasimistic approach - don't let problems arise in teh first place
                        How to know is an execution schedule is correct without knowing the entire schedule ahead of time?
                        2PL (pasimistic) - concurency control protocol which guarantee conflict serialivability but is also subject to cascading aborts and limits concurency since guaranties more than needed (gives conflict serializability while needed only view serializability).
                            in which there are two phases:
                                take locks
                                execute trx ops and release locks
                            Strict 2PL (solves cascading aborts) - release all locks only at the very end of the transaction
                            Deadlocks prevention:
                                detection - create a "wait for" graph of trxs and periodically check for cycles in that graph. then decide how to break the cycle (choose victim, abort or partial rollback).
                                prevention - kill emidiatelly one of the trxs when conflict arises.
                            Lock types:
                                Shared - tuple read
                                Exclusive - tuple write
                                Intensive Shared - tuple aggregation unit locked and all children of it are locked for read
                                Intencive Exclusive - tuple aggregation unit locked and all children of it are locked for write
                                Shared Intencive Exclusive - tuple aggregation unit locked and all children of it are locked for read while some are locked also for wtire
                    Optimistic approach - assume conflicts are rare and deal with them after they happen
                        Basic T/O (Timestamp Ordering) - use timestamps to determine the serializability order of txns. each tuple should have two timestamps one for the most recent read and one for the most recent write. then old TS txn gets aborted if it tries to read or write a value "from the future"
                            high overhead from copying data to txn's workspace and from updating timestamps
                            long txns can be starved
                        OCC - Optimistic concurrency control:
                            Read phase - read all tuples the txn needs and save in local worspace of the txn
                            Validate phase - when txn commits check whether it conflicts with other txns. the txn gets a timestamp only at this phase.
                                validation is the bottleneck of the protocol, it needs latches to be consistent
                            Write phase - if validation succeeds apply private changes to database, otherwise abort the txn
                        Partition-based T/O - divide the DB horizontally into partitions - good for workloads that needs txns that manipulate within a single partition. no need for data copying since only one txn can run insode a partition. abort if partition lock is held by other.
                    MVCC - Multi-Version concurency control - assign timestamps only on writes and maintain virtual snapshots of the whole DB. 
                        CC protocol (listed above)
                        storage models:
                            append only storage - append modified tuple to the first empty slot of the table and update a pointer from the older version to the new one (or vice versa O2N or N2O).
                            time-travel storage - per table have time-travel table with the full tuples history.
                            delta storage - per table have delta table with only delta history.
                        Garbage collection approaches:
                            tuple level - each tuple has versions and GC clears periodically or "cooperatively" (lazy - on other operation on the tuple.)
                            transaction level - each txn keeps track of it's read/write set and DBMS determines when all versions created by the txn are no longer visible.
                        Index management:
                            secondery points to primery
                            secondery points to hash table
                            secondery holds phisical address (page and offset)
        Isolation:
        Durability:
            Steel policy - a non-commited transaction is allowed to "steel" a page from an already commited transaction (each one uses different tuples but the tuples are on the same page). Non-steel policy is the oposite.
            force policy - force all txn's changes be flushed to no-volatole storage before the commit (all pages). non-force is the oposite.
            non-steel + force: easiest way to guarantee durability. the problem is all pages of the txn must persist in RAM during the txn run-time. this can be handled by:
                Shadow paging - copy on write pages with write txn operation to temporary place and have a shadow page table pointing to it. once the txn finished switch the page table pointer to point the shadow table such that the master table becomes the shadow one for the next txns.
                this is bad idea
            steel + no-force: log BEGIN, ACTIONS and COMMIT into a WAL (write ahead logs) ACTIONS are: txnId, objId, before val and after val
                should have two log buffers - when one is hardened, the second one is already written in RAM. TODO Kopzon - why flushing after timeout (which's size is the flush duration helps?)
                logical logging - save high level info about the changes to the DB
                Checkpoints - log marker that says all buffer pool was flushed to disk, hence all commited txns are on disk, so log can be garbage collected.
            crash recovery:
                writing log records:
                    pageLSN - the LSN (log sequence number) of the most recent update for a data page (page can be flushed only after the log corresponding to it's LSN is already flushed)
                    flushLSN - the most recent flushed LSN. update every time the DBMS erites a WAL (write ahead log) page to memory
                    recLSN - on each page record. the LSN of the log record that first caused the pade to be dirty (brought the page into memory for write)
                    prevLSN - on each log record the previous LSN or a given transaction (makes your life easier to track all transactions actions during UNDO phase)
                    CLR - (compensation log records) log records that describe an UNDO operation after abort. undoNextLSN - the next to-be-undone LSN. CLR never need to be undone (TODO Kopzon - understand it again)
                    TRANSACTION END - a new log record that says "this transaction hardened everything to this point of the log at recovery". printed to the log after the last CLR on abort flow
                    fuzzy checkpoints:
                        CHACKPOINT BEGIN and CHECKPOINT END (with ATT{active transaction table} and DPT{dirty page table}).
                        LSN of the CHECKPOINT BEGIN is hardened when the chackpoint finishes. any txn that starts after the checkpoint is axcluded from ATT
                the recovery itself:
                    Analysis - scan the WAL from the CHECKPOINT START till the end and figure out the txns than failed or commited since then 
                    Redo - go to smallest recLSN in DPT and redo (reapply and set pageLSN to curr record's LSN) all (unless affected page is not in DPT or page recLSN is higher then current record's LSN) log records from than till the end of the log. At the end write TXN END and remove from ATT all comitted transactions
                    Undo - undo all the log records of failed txns (aborted or not commited after redo) from the end till the oldest log record of any txn from ATT. write CLR after each undo operation for recovery after crash during recovery.
                    
                    
