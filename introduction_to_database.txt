leacture 1:
-----------
-----------

DBMS - database management system - allows definition, creation, querying, update and administration of databases
DML - data manipulation languages:
    Procedural (Relational Algebra) - the query specifies the high level strategy the DBMS should use to find the desired result.
        Relational algebra operators:
            Select
            Projection
            Union
            Intersection
            Difference
            Product
            Join
        Extra operators:
            Rename
            Assign
            Aggregate
            Sort
            Division
    Non-Procedural (Declarative) (Relational Calculus) - the query specifies only what data is wanted and not how to find it


leacture 2:
-----------
-----------

SQL - Structured Query Language
-------------------------------
DML - Data Manipulation Language - actions on data
DDL - Data Definition Language - creating tables
DCL - Data Control Language - security and permissions
Aggregates - can only be used in the SELECT output list
    AVG(col)
    MIN(col)
    MAX(col)
    SUM(col)
    COUNT(col)
    example:
        SELECT COUNT(login) AS cnt FROM student WHERE login LIKE '@sc';
        SELECT COUNT(*) AS cnt FROM student WHERE login LIKE '@sc';
        SELECT COUNT(1) AS cnt FROM student WHERE login LIKE '@sc'; TODO Kopzon - why all 3 are the same?
    GROUP BY
        Non aggregated values in SELECT output, must appear in GROUP BY clause
        example:
            SELECT AVG(s.gpa), e.cid, s.name FROM enrolled AS e, student AS s WHERE e.sid = s.sid GROUP BY e.cid, s.name;
    HAVING
        Filters results based on aggregation computation (like WHERE clause for a GROUP BY)
        example:
            SELECT AVG(s.gpa) AS avg_gpa. e.cid FROM enrolled AS e, student AS s WHERE e.sid = s.sid GROUP BY e.cid HAVING avg_gpa > 3.9;
String operations:
    LIKE
        % - matches any substring (including empty ones)
        _ - matches aby one char
        example SELECT * FROM student AS s WHERE s.login LIKE 'kookoo%bomba_yal'
    SUBSTRING
        SELECT SUBSTRING(name,0,5) AS abbrv_name FROM student WHERE sid = 55843
    UPPER/LOWER
        SELECT * FROM student as s WHERE UPPER(e.name) LIKE 'KAN%'
    Concatinate
        ||
        +
        CONCAT(,)
Date and Time operatiopns:
    veries in diferent flavores of sequl
Output redirection:
    create new table
        SELECT DISTINCT cid INTO CourseIds FROM ENROLLED;
    add to existing table
        INSERT INTO CourseIds (SELECT DISTINCT cid FROM enrolled)
    ORDER BY <column> [ASC|DESC]
        SELECT sid FROM enrolled WHERE cid = '15-721' ORDER BY grade DESC, sid ASC (note that grade is not even part of the output)
    LIMIT <count> OFFSET [offset]
Window Functions:
    Like an aggregation but tuples are not grouped into a single output (you get the whole tuple in the output).
        SELECT ... FUNC-NAME(...) OVER(...) FROM tableName
        where FUNC-NAME can be:
            Aggregation functions...
            ROW_NUMBER() - # of the current row
            RANK() - order of the current row
Common table expressions
    useful to avoid query nasting.
        WITH cteName (col1, col2) AS (SELECT ...) SELECT * FROM cteName - col1 and col2 and bind to the temp table columns names
    can be used for recusrions:
        WITH RECURSIVE cteSource (counter) AS (
            (SELECT 1)
            UNION ALL
            (SELECT counter + 1 FROM cteSource WHERE counter < 10)
        )
        SELECT * FROM cteSource;

Memory Management
-----------------
Why not using mmap()
    The OS doesn't know what really does the application do, so swap stalls are very heavy. A good workaround for this issue is:
        madvice() - tell the OS how you expect to read the pages
        mlock() - mark memory ranges that cannot be paged out
        msync() - flush memory
    still this isn't a good solution for multiple writers. (Full usage - monetDC, LMDB, ravenDB, levelDB elasticsearch)
    OS is general purpose track, while our system is a Porche or Ferrari
    TODO Kopzon - managing our own file system in a big pain in the ass and can give not more than 10% performance enhancement. (is it true?)

How the DBMS represents the DB in files on disk?
    Page contains unique sort of data (tuples, meta-data, indexes, log records...)    
    indirection layer to map page id to file and offset.
    DB page size is 512B-16KB (ORACLE, IBM DB2 and SQLite - 4KB) (MySQL - 16KB high end) high end systems usually give choice with DB page size per type
    Hardware pages are 4KB atomic
    page directory - lookup table for pages on disk
    page contents:
        Header:
            page size
            checksum
            BDMS version
            Transaction visibility TODO Kopzon - what is it?
            Compression info
        1st option:
            Slot array - mapping layer from a slot to a tuple (slot array growing from beginning to the end and tuples grow from end to beginning)
            Tuples
        2nd option:
            Log structured - Append records to a log file when DB is modified. (good for writes, bad for reads, but you can periodically build indexes and compact logs)
                Insert - store the entire tuple
                Delete - mark the tupl as deleted
                Update contain the delta of the modified attribute
    GC compacts the tuples in a page
    tuple contents:
        header
        attributes
        BLOB - Binary Large OBject - is a tuple which's content is too big. Can be handles by:
            few pages with indirection (like a list of pages)
            within an external file (DBMS can't manipulate these files)
    normalization - the way you split up the DB across different tables
    denormalization - pre joining - create a join table instead of two tables (cloud spanner from Google - protobuf nested tables use denormalization)
    Table schema - meta data of a table in the DB (can sit inside the DB itself or rely on external FS)
    Transaction types:
        OLTP - Online transaction processing - simple mostly write queries, small amount of data involved
        OLAP - Online analytical processing - complex mostly read queries, large amount of data
        HTAP - Hybrid transactional analytical processing
    mongoDB, bigTable and kasandra are OLTP optimized and are noSQL systems and this is why the are able to scale TODO Kopzon - why and what does it mean?
    storage models:
        n-ary (row store) - all the attributes of a single tuple are stored continuesly in a single page - best for OLTP workloads
        DSM (column store) - Decomposition Storage Model - best for OLAP workloads (analytics). How do we match different attributes of sam tuple?
            fixed length offsets - issue with variable length attributes
            embedded tuple ids - waste of space
        get 4KB page from SSD - 150micro, scan 4KB in INT32 jumps is 33micro (3Ghz processor, 10clock cycles per loop)? TODO Kopzon - check
        It is a common solution to store the DB with a row transactional store (OLTP) for real time transaction based applications and when the data starts to be less relevant (in time for example) it is copied to a backend column analytical store (OLAP).

How the DBMS manages it's memory and moves data back and forth from disk?
    allocate a memory region called buffer cache (frame pool) and bring needed pages from dist to it.
    Use page table to map page to frame in the pool.
    save dirty and reference bits on frames to control correct swapping.
    lock - high level transaction syncronization mechanism (should be able to roll back transactions etc...)
    latch - low level lock for protecting data resources from multi-threaded corruption.
    page directory - where is the page in the disk (must be durable)
    page table - where is the page in the buffer pool (don't have to be durable)
    pre-fetching - OS prefetchs pages using simple locality heuristic. The DBMS can pre-fetch non-sequential pages based on table indexes and query requirements.
    scan sharing - allow multiple queries attach to a single table cursor that scans the table. (queries don'thave to be exactly the same)
    result caching - save query result in cache to output it on demand for the same query in the future
    buffer cache bypass - for sort/ large sequential queries - allocate temporary memory for pages for the specific query to reduce buffer cache cooling (with regard to other queries that use the buffer cache and made it "hot" for them)
    most DBMS use O_DIRECT (direct IO) to avoid using the OS page cache.
    page replacement policy
        LRU
            time stamp per page in page table + scan all pages in page table vor every page removal operation (wase of time to iterrate)
            queue - add to the end, remove from the beginning + reinsert all pages that are reused (the queue lock is contended)
            clock - approximation of LRU - hold all pages in a cyrcular buffer. if page is referenced, update bRef to 1. when page should be swapped out the if the clock arrow points to a page with bRef = 1, assign bRef = 0 and go to the next one, otherwise swap this page. (no need for taking lock on cache hit, just update bRef)
            note:
                sequential flooding - cache trashing on one sequential query
        LRU-K
            track the lask K references to each page and remove from cache the one with the biggest AVG interval between timestamps
        Localization per query - hold page cache per query and not a global one
        Priority Hints - knowing the query structure we can save in cache the most frequently needed pages for the query

Hash Tables:
    Linear Search - on collision put the new key at the first available slot after it's optimal slot.
        Robin Hood hashing - each inserted key stores it's distance from it's optimal slot, and if new key's distance is greater then the old in the slot, they are swapped.
        CuKoo Hashing - multiple hash functions and hash tables. insert to the hash that doesn't have a collision. If there is a collision, replace the old key and reinsert it to another hash. Do it in a loop till you reinsert all key's or recognize you need to extent the hash table size.
        In case of resize we have to rebuild the entire table
    Chain hashing - each entry maps to a bucket of keys.
    Extendible hashing - Like chain hashing, but the pointers table  has a global counter that represents the amount of  bits to we have to look at when searcing for bucket (page) with data. When a bucket is getting full, we split only this bucket, double the pointer's table size, increasing the global bit counter and update the pointers (updating only 3 pages for resize)
    Linear haashing - splitting buckets based on the split pointer, double the slot array, add new hash function with modulo X2 from the previous hash function and remove the old hash function when the split pointer gets to the end of the slot array. To find a key use the first hash function and if the result is above the split pointer we may need to use the second hash function. split pointer can move back and slot arary can shrink by two when a splitted bucket gets empty
    
    

leacture 3:
-----------
-----------

leacture 4:
-----------
-----------

leacture 5:
-----------
-----------

leacture 6:
-----------
-----------

leacture 7:
-----------
-----------

leacture 8:
-----------
-----------

leacture 9:
-----------
-----------

leacture 10:
-----------
-----------

leacture 11:
-----------
-----------

leacture 12:
-----------
-----------

leacture 13:
-----------
-----------

leacture 14:
-----------
-----------

leacture 15:
-----------
-----------

leacture 16:
-----------
-----------

leacture 17:
-----------
-----------

leacture 18:
-----------
-----------

leacture 19:
-----------
-----------

leacture 20:
-----------
-----------

leacture 21:
-----------
-----------

leacture 22:
-----------
-----------

leacture 23:
-----------
-----------

leacture 24:
-----------
-----------

leacture 25:
-----------
-----------

leacture 26:
-----------
-----------
